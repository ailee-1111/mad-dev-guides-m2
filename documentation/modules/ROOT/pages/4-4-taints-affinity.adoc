= 노드 선택기, taint
전용 노드에서 실행되도록 워크로드를 구성하고 다른 워크로드가 해당 노드에서 실행되지 못하도록 합니다.

== 노드의 워크로드 보장 및 방지
OpenShift 고급 예약 기능에는 포드가 특정 노드에 배치되도록 하는 노드 선택기와 특정 노드에서 일부 워크로드를 실행하지 못하게 하는 taint 및 toleration이 포함됩니다.

노드 선택기를 사용하여 전용 노드 집합에 포드를 예약합니다. 예를 들어 포드에 필요한 특수 하드웨어를 제공하는 노드에 포드를 예약할 수 있습니다.

taint 및 toleration을 사용하여 전용 노드 집합에 포드를 예약하지 않도록 합니다. 예를 들어 OpenShift 클러스터 서비스 또는 컨트롤 플레인 서비스용으로 예약된 노드에서 실행되지 않도록 포드를 차단합니다.

== 노드 선택기
노드 선택기를 사용하면 포드에서 대상 노드 레이블을 지정할 수 있으므로 OpenShift 스케줄러는 대상 레이블과 일치하는 노드에만 포드를 배치하려고 합니다. OpenShift 스케줄러에서 대상 레이블과 일치하는 노드를 찾지 못하면 포드를 사용할 수 없습니다.

포드 노드 선택기를 사용하여 전용 노드에 포드를 배치할 수 있습니다. 포드 노드 선택기를 포드 템플릿의 속성으로 정의합니다.

프로젝트 노드 선택기를 사용하여 전용 노드에 프로젝트의 포드를 배치할 수 있습니다. 프로젝트 노드 선택기를 프로젝트 CR에서 주석으로 정의합니다.

클러스터 전체 노드 선택기를 사용하여 클러스터의 임의 위치에서 전용 노드에 포드를 배치합니다. 클러스터 전체 노드 선택기를 OpenShift 스케줄러 CR의 속성으로 정의합니다.

image::2-7.png[2-7]

이전 다이어그램은 포드 노드 선택기의 작동 방식을 보여줍니다. 그림에서 worker01 노드에는 key1=value1 레이블이 있지만 worker02 노드에는 해당 레이블이 없습니다. 첫 번째 포드의 경우 OpenShift는 노드 선택기가 없으므로 두 노드 중 하나에서 포드를 예약할 수 있습니다.

두 번째 포드에는 포드 템플릿에서 속성으로 정의된 노드 선택기가 있습니다. 그러면 OpenShift는 노드 선택기와 일치하는 레이블이 있는 노드에서만 포드를 예약할 수 있습니다. 따라서 OpenShift 스케줄러는 worker01 노드에만 포드를 배치할 수 있습니다.

* *노드 레이블*
OpenShift 스케줄러가 대상 레이블과 일치하는 노드를 찾도록 하려면 먼저 노드에 레이블을 지정합니다. 레이블을 노드 리소스에 직접 추가하거나 시스템 집합 리소스를 사용하여 간접적으로 추가할 수 있습니다.

클러스터가 시스템 집합 리소스를 지원하는 방식으로 설치된 경우 시스템 집합을 사용하여 노드에 레이블을 설정해야 합니다. 이 경우 노드에 직접 레이블을 지정하지 마십시오. 클러스터가 사용자 프로비저닝된 인프라 방법을 사용하여 설치된 경우와 같이 시스템 집합 리소스를 지원하지 않는 경우 노드에 직접 레이블을 지정해야 합니다.

TIP: 
클러스터에서 시스템 집합 리소스를 지원하는 경우 OpenShift는 언제든지 노드를 삭제하고 다시 생성할 수 있습니다. 따라서 노드에 직접 레이블을 지정하면 OpenShift에서 다시 생성할 때 시스템 집합 외부에서 노드의 레이블이 손실됩니다.

다음 명령을 사용하여 노드에 하나 이상의 레이블을 추가할 수 있습니다.

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[user@host ~]$ oc label node node1 key1=value1 ... keyN=valueN
node/node1 labeled
----

다음 명령을 사용하여 노드에서 레이블을 삭제할 수 있습니다.

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[user@host ~]$ oc label node node1 key1- ... keyN-
node/node1 unlabeled
----

* *포드 노드 선택기* 
포드에서 노드 선택기를 사용하여 OpenShift 스케줄러에서 포드를 배치하려는 대상 노드 레이블을 지정합니다.

다음 구문을 사용하여 포드에 하나 이상의 노드 선택기를 추가할 수 있습니다.

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
  ...output omitted...
spec:
  nodeSelector:
    key1: value1
    key2: value2
...output omitted...
----

이전 예제에서 OpenShift 스케줄러는 key1=value1 및 key2=value2 레이블이 모두 있는 노드에만 포드를 배치합니다.

다음과 같이 Node-Selectors 매개 변수를 검토하여 포드의 노드 선택기를 읽을 수 있습니다.

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[user@host ~]$ oc describe pod myapp-95664666d-5l22c
...output omitted...
Node-Selectors:     key1:value1
                    key2:value2
...output omitted...
----

* *클러스터 전체 노드 선택기*
기본적으로 OpenShift 스케줄러는 클러스터에서 사용 가능한 모든 노드에 포드를 배치하려고 합니다. 그러나 클러스터에 생성된 새 포드에 기본 노드 선택기를 포함하도록 OpenShift 스케줄러 기본 동작을 수정할 수 있습니다. 클러스터 전체 노드 선택기를 사용하여 OpenShift에서 포드에 추가하는 기본 노드 선택기를 지정합니다. 클러스터 전체 노드 선택기를 지정하려면 관리 권한이 필요합니다.

클러스터 전체 노드 선택기를 정의하려면 다음과 같이 OpenShift 스케줄러 CR을 편집할 수 있습니다.

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[user@host ~]$ oc edit scheduler cluster
----

그런 다음 defaultNodeSelector 매개 변수를 지정합니다.

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
apiVersion: config.openshift.io/v1
kind: Scheduler
metadata:
  name: cluster
...output omitted...
spec:
  defaultNodeSelector: key1=value1,key2=value2
...output omitted...
----

OpenShift 스케줄러 CR을 편집한 후 openshift-kube-apiserver 프로젝트의 포드가 재배포될 때까지 기다립니다.

이전 예제에서 클러스터에 생성하는 모든 새 포드에는 기본적으로 key1=value1 및 key2=value2 노드 선택기가 있습니다.

* *프로젝트 노드 선택기* 
프로젝트에 대한 노드 선택기를 지정할 수 있으므로 OpenShift는 프로젝트에 생성된 새 포드에 해당 노드 선택기를 포함합니다. 프로젝트 노드 선택기를 지정하려면 관리 권한이 필요합니다.

프로젝트 노드 선택기를 정의하려면 OpenShift 프로젝트 CR에 대한 YAML 파일을 생성하고 다음과 같이 metadata.annotations 섹션에서 노드 선택기를 정의할 수 있습니다.

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
apiVersion: v1
kind: Namespace
metadata:
  name: myproject
  annotations:
    openshift.io/node-selector: key3=value3,key4=value4
...output omitted...
----

이전 예제에서 프로젝트에 생성하는 모든 새 포드에는 기본적으로 key3=value3 및 key4=value4 노드 선택기가 있습니다.

* *노드 선택기 수준*
포드, 프로젝트 및 클러스터 전체 수준에서 노드 선택기를 정의할 수 있습니다.

프로젝트 노드 선택기는 클러스터 전체 노드 선택기보다 우선합니다. 따라서 프로젝트 및 클러스터 전체 노드 선택기를 모두 정의하는 경우 OpenShift는 프로젝트 노드 선택기만 새 포드에 적용하고 클러스터 전체 노드 선택기는 적용하지 않습니다.

포드 노드 선택기는 클러스터 전체 및 프로젝트 노드 선택기에 추가됩니다. 따라서 포드 및 프로젝트 노드 선택기를 모두 정의하는 경우 OpenShift는 포드 노드 선택기와 프로젝트 노드 선택기가 모두 있는 포드를 생성합니다. 포드 및 클러스터 전체 노드 선택기를 모두 정의하는 경우 OpenShift는 포드 노드 선택기와 클러스터 전체 노드 선택기가 모두 있는 포드를 생성합니다.

IMPORTANT: 
동일한 key 문자열을 사용하지만 프로젝트 노드 선택기 또는 클러스터 전체 노드 선택기와 다른 value 문자열을 사용하여 포드 노드 선택기를 생성하면 충돌이 발생하고 OpenShift에서 포드를 생성하지 못합니다.



== taint 및 toleration
노드에서 taint를 사용하여 특정 노드에서 일부 워크로드를 실행하지 못하게 할 수 있습니다. OpenShift 스케줄러는 포드에 일치하는 toleration이 있는 경우에만 해당 노드에서 포드를 예약합니다.

image::2-8.png[2-8]

위 그림에서 worker01 노드에는 taint가 있지만 worker02 노드에는 taint가 없습니다. OpenShift 스케줄러는 worker01 노드에 첫 번째 포드를 배치할 수 없습니다. toleration이 taint와 일치하지 않으므로 노드에서 포드를 거부합니다. 따라서 OpenShift는 worker02 노드에서만 포드를 예약할 수 있습니다.

OpenShift는 toleration이 taint와 일치하므로 두 노드 모두에서 두 번째 포드를 예약할 수 있습니다.

=== 노드 taint
taint는 키, 값 및 효과로 구성됩니다.

키는 최대 253자의 임의 문자열입니다.

값은 최대 63자의 임의 문자열입니다.

효과는 다음 옵션 중 하나입니다.

* *NoSchedule*
포드가 taint와 일치하지 않으면 노드에서 포드가 예약되지 않도록 합니다. 노드의 기존 포드는 해당 노드에서 계속 실행됩니다.

* *PreferNoSchedule*
포드가 taint와 일치하지 않으면 스케줄러는 노드에서 포드를 예약하지 않으려고 시도합니다. 노드의 기존 포드는 해당 노드에서 계속 실행됩니다.

* *NoExecute*
포드가 taint와 일치하지 않으면 노드에서 포드가 예약되지 않도록 합니다. 스케줄러는 일치하는 toleration이 없는 노드의 기존 포드를 제거합니다.

`spec.taints` 매개 변수를 설정하거나 다음 명령을 사용하여 노드 사양 YAML 파일의 노드에 taint를 적용할 수 있습니다.

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[user@host ~]$ oc adm taint nodes node1 key1=value1:NoSchedule
node/node1 tainted
----

노드에 taint를 적용한 후 다음 명령을 사용하여 taint를 확인할 수 있습니다.

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[user@host ~]$ oc describe nodes node1
Name:               worker01
Roles:              worker
...output omitted...
Taints:             key1=value1:NoSchedule
...output omitted...
----

노드에서 taint를 제거하려면 다음 명령을 사용합니다.

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[user@host ~]$ oc adm taint nodes node01 key1=value1:NoSchedule-
node/node01 untainted
----


=== 포드 toleration
toleration은 노드 taint와 유사하게 키, 값 및 효과로 구성됩니다. toleration에는 operator 매개 변수도 포함됩니다. operator 매개 변수에 사용할 수 있는 값은 다음과 같습니다.

* *Equal*
키, 값, 효과 매개 변수가 일치하는 경우 toleration이 taint와 일치합니다. 이는 기본 동작입니다.

* *Exists*
키 및 값 매개 변수가 일치하는 경우 toleration이 taint와 일치합니다. 값 매개 변수를 비워 두어야 합니다.

NoExecute 효과를 사용하여 toleration을 정의하는 경우 tolerationSeconds 매개 변수도 정의할 수 있습니다. tolerationSeconds 매개 변수는 노드에서 포드를 제거하기 전에 포드가 노드 내에 유지되는 시간을 정의합니다.

다음과 같이 spec.tolerations 매개 변수를 설정하여 포드 사양 YAML 파일의 포드에 toleration을 적용할 수 있습니다.

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
  ...output omitted...
spec:
  tolerations:
  - key: "key1"
    operator: "Equal"
    value: "value1"
    effect: "NoExecute"
    tolerationSeconds: 3600
...output omitted...
---- 
OpenShift는 key1=value1:NoExecute taint를 사용하여 노드에서 포드를 예약할 수 있습니다. OpenShift가 taint가 있는 노드에서 포드를 예약하는 경우 노드에서 3600초 동안 포드가 실행된 후, 노드가 포드를 제거하고 OpenShift에서 포드를 다른 노드로 다시 예약합니다.

*  *포드 예약 및 노드 조건*
기본적으로 OpenShift는 조건에 따라 노드를 taint하고 제거합니다. 예를 들어 OpenShift는 메모리 또는 디스크 압력과 같은 일부 조건에서 노드를 자동으로 taint합니다.

다음 taint가 OpenShift에 빌드됩니다.

- `node.kubernetes.io/not-ready`
노드가 준비되지 않았습니다. 이 taint는 Ready=False 노드 조건에 해당합니다.

- `node.kubernetes.io/unreachable`
노드 컨트롤러에서 노드에 연결할 수 없습니다. 이 taint는 Ready=Unknown 노드 조건에 해당합니다.

- `node.kubernetes.io/memory-pressure`
노드에 메모리 압력 문제가 있습니다. 이 taint는 MemoryPressure=True 노드 조건에 해당합니다.

- `node.kubernetes.io/disk-pressure`
노드에 디스크 압력 문제가 있습니다. 이 taint는 DiskPressure=True 노드 조건에 해당합니다.

- `node.kubernetes.io/network-unavailable`
노드 네트워크를 사용할 수 없습니다.

- `node.kubernetes.io/unschedulable`
노드를 예약할 수 없습니다.

- `node.cloudprovider.kubernetes.io/uninitialized`
이 taint는 외부 클라우드 프로바이더에서 노드 컨트롤러를 시작할 때 노드를 사용할 수 없는 것으로 설정합니다. 클라우드 컨트롤러 관리자가 노드를 초기화하면 kubelet에서 taint를 제거합니다.

- `node.kubernetes.io/pid-pressure`
노드에 PID(프로세스 식별자) 압력이 있습니다. 이 taint는 PIDPressure=True 노드 조건에 해당합니다.

노드가 위 표의 조건 중 하나를 보고하면 OpenShift는 조건이 지워질 때까지 노드에 taint를 적용합니다.

예를 들어 다음 예제에 따라 OpenShift에서 예약할 수 없는 포드에 node.kubernetes.io/unschedulable taint를 자동으로 적용하는 방법을 확인할 수 있습니다. 먼저 다음 명령을 사용하여 노드의 taint를 확인합니다.

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[user@host ~]$ oc describe nodes node1 | grep Taints
Taints:     <none>
----

노드를 예약할 수 없음으로 표시합니다.

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[user@host ~]$ oc adm cordon node1
node/node1 cordoned
----

마지막으로 OpenShift에서 NoSchedule 효과가 있는 node.kubernetes.io/unschedulable taint를 노드에 자동으로 적용하는지 확인합니다.

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[user@host ~]$ oc describe nodes node1 | grep Taints
Taints:     node.kubernetes.io/unschedulable:NoSchedule
----

OpenShift는 NoSchedule 효과를 사용하여 이전 표의 노드에 taint를 적용합니다. 단, OpenShift에서 NoExecute 효과를 사용하는 node.kubernetes.io/not-ready 및 node.kubernetes.io/unreachable taint는 제외됩니다.

NoSchedule 효과가 있는 taint의 경우 포드에 일치하는 toleration이 있거나 노드가 정상 상태로 돌아가지만 실행 중인 포드가 변경되지 않은 상태로 남아 있는 경우 OpenShift 스케줄러는 포드를 노드에 예약합니다.

NoExecute 효과가 있는 taint의 경우 포드에 일치하는 toleration이 있거나 노드가 정상 상태로 돌아가지만 실행 중인 포드를 제거하고 다른 노드에 다시 예약하는 경우 OpenShift 스케줄러는 포드를 노드에 예약합니다. taint를 허용하지 않고 노드에서 실행 중인 포드의 경우 OpenShift에서 해당 포드를 즉시 제거합니다. toleration과 일치하는 포드의 경우 tolerationSeconds 매개 변수가 포드에 정의된 경우에만 OpenShift에서 해당 포드를 제거합니다. tolerationSeconds 매개 변수는 노드에서 포드를 제거하기 전에 포드가 노드 내에 유지되는 시간을 초 단위로 정의합니다.

tolerationSeconds 매개 변수는 애플리케이션을 처음 실행할 때 로컬 파일을 생성해야 하는 경우와 같은 특정 시나리오에서 유용합니다. 이 경우 포드가 복구될 때까지 몇 초 동안 기다리는 것이 다른 노드에서 포드를 직접 다시 생성하는 것보다 나을 수 있습니다. 애플리케이션에 대해 tolerationSeconds 매개 변수를 정의하는 경우 노드는 포드를 제거하기 전에 지정된 시간 동안 기다립니다.

== 프로젝트 toleration
프로젝트에 대한 toleration을 지정할 수 있으므로 OpenShift는 프로젝트에 생성된 새 포드에 해당 toleration을 포함합니다. 프로젝트 toleration을 지정하려면 관리 권한이 필요합니다. 프로젝트 및 포드 toleration을 정의하면 OpenShift에서 이러한 toleration을 모두 사용하여 포드를 생성합니다.

프로젝트 toleration을 정의하려면 OpenShift 프로젝트 CR에 대한 YAML 파일을 생성하고 다음과 같이 metadata.annotations 섹션에서 toleration을 정의할 수 있습니다.

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
kind: Project
apiVersion: project.openshift.io/v1
metadata:
  name: myproject
  annotations:
    scheduler.alpha.kubernetes.io/defaultTolerations: >-
      [{"operator":"Equal","effect":"NoSchedule","key":"key1","value":"value1"}]
----

이전 예제에서 프로젝트에 생성하는 모든 새 포드에는 기본적으로 key1=value1:NoSchedule toleration이 있습니다.

=== 모든 taint 허용
키 또는 값 매개 변수 없이 operator: "Exists" toleration을 사용하여 모든 노드 taint를 허용하도록 포드를 구성할 수 있습니다.

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
...output omitted...
spec:
  tolerations:
  - operator: "Exists"
----


= Taints and Affinity

지금까지 Kubernetes 클러스터에 Pod를 배포하면 요구 사항(예: 메모리 요구 사항, CPU 요구 사항 등)을 충족하는 모든 노드에서 실행되었습니다. 그러나 Kubernetes에는 다음을 추가로 구성할 수 있는 두 가지 개념이 있습니다.  일부 비즈니스 기준에 따라 Pod가 노드에 할당되도록 스케줄러를 사용합니다.

== Preparation



[#kubectl-deploy-app]
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc new-project taint-%userid%
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
namespace/taint-%userid% created
----

NOTE: `oc new-project taint-%userid%` : taint-%userid%라는 새 프로젝트(네임스페이스)를 생성합니다.

[#kubectl-deploy-app]
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc project taint-%userid%
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
Now using project "taint-%userid%" on server "https://172.30.0.1:443".
----

NOTE: `oc project taint-%userid%` : 현재 활성화된 컨텍스트의 기본 네임스페이스를 taint-%userid%로 변경합니다.



네임스페이스에서 아무것도 실행되고 있지 않은지 확인하세요.

[#no-resources-resource]
[.console-input]
[source, bash]
----
oc get all
----

[.console-output]
[source,bash]
----
No resources found in myspace namespace.
----


=== Watch Nodes


무슨 일이 일어나고 있는지 관찰할 수 있도록 다른 Terminal#2를 열고 다양한 작업을 실행할 때 어떤 일이 일어나는지 `관찰`해 보겠습니다.


===Terminal#2에서 작업


[.console-input]
[source,bash,subs="+macros,+attributes"]
----
watch -n 1 "oc get pods -o wide \
  | awk '{print \$1 \" \" \$2 \" \" \$3 \" \" \$5 \" \" \$7}' | column -t"
----


TIP: `-o wide` 옵션을 사용하면 포드가 예약된 노드를 볼 수 있습니다. +
줄이 너무 길어지는 것을 방지하기 위해 `awk`와 `column`을 사용하여 원하는 열만 가져오고 형식을 지정합니다.



== Taints

특정 Pod를 예약하거나 예약하지 않도록 스케줄러에 신호를 보내는 Kubernetes 노드에 Taint가 적용됩니다. +
Toleration은 Pod 정의에 적용되고 Taint에 예외를 제공합니다. 현재 노드를 설명하겠습니다. +
이 경우 OpenShift 클러스터는 다음과 같습니다. 


[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc describe nodes | egrep "Name:|Taints:"
----

[.console-output]
[source,bash]
----
Name:               ip-10-0-136-107.eu-central-1.compute.internal
Taints:             node-role.kubernetes.io/master:NoSchedule
Name:               ip-10-0-140-186.eu-central-1.compute.internal
Taints:             <none>
Name:               ip-10-0-141-128.eu-central-1.compute.internal
Taints:             <none>
Name:               ip-10-0-146-109.eu-central-1.compute.internal
Taints:             <none>
Name:               ip-10-0-150-226.eu-central-1.compute.internal
Taints:             <none>
----

[NOTE]
====
이 경우 '마스터' 노드에는 애플리케이션 포드가 예약되는 것을 차단하는 오염이 포함되어 있습니다.
====

모든 노드에 taint를 추가해 보겠습니다.

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl taint nodes --all=true color=blue:NoSchedule
----

[.console-output]
[source,bash]
----
node/ip-10-0-136-107.eu-central-1.compute.internal tainted
node/ip-10-0-140-186.eu-central-1.compute.internal tainted
node/ip-10-0-141-128.eu-central-1.compute.internal tainted
node/ip-10-0-146-109.eu-central-1.compute.internal tainted
node/ip-10-0-150-226.eu-central-1.compute.internal tainted
node/ip-10-0-155-122.eu-central-1.compute.internal tainted
node/ip-10-0-162-206.eu-central-1.compute.internal tainted
node/ip-10-0-168-102.eu-central-1.compute.internal tainted
node/ip-10-0-175-64.eu-central-1.compute.internal tainted
----
color=blue는 단순히 오염을 식별하기 위한 키=값 쌍이고 NoSchedule은 오염을 "허용"할 수 없는 포드에 대한 특정 효과입니다.  즉, Pod가 "color=blue"를 허용하지 않으면 효과는 "NoSchedule"이 됩니다.


그럼 이것을 시험해 봅시다.  기본 터미널에서 특별한 허용 사항이 없는 새 포드를 배포합니다.



== Terminal#1

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
cat <<EOF | oc create -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: myboot
  name: myboot
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myboot
  template:
    metadata:
      labels:
        app: myboot
    spec:
      containers:
      - name: myboot
        image: quay.io/rhdevelopers/myboot:v1
        ports:
          - containerPort: 8080
EOF
----


Terminal#2의 출력이 변경되는 것을 볼 수 있습니다.

[.console-output]
[source,bash,subs="+quotes"]
----
NAME                      READY   STATUS    AGE     NODE
myboot-7cbfbd9b89-hqx6h   0/1     #Pending#   4m12s   devnation
----


사용 가능한 예약 가능한 노드가 없으므로 포드는 'Pending' 상태로 유지됩니다.

다음을 입력하면 이에 대한 더 많은 통찰력을 얻을 수 있습니다.


Terminal 1
+
--
// 태그가 지정되지 않은 모든 지역과 openshift로 태그가 지정된 모든 지역을 포함합니다.
// See: https://docs.asciidoctor.org/asciidoc/latest/directives/include-tagged-regions/#tagging-regions
include::partial$taint-remove-taint.adoc[tags=**;!*;openshift]


// tag::openshift[]
:chosen-node: ip-10-0-140-186.eu-central-1.compute.internal
// end::openshift[]



[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc describe pod #<.>
----
<.> 이 경우 포드는 하나만 있습니다.  구체적으로 지정하려면 포드 이름을 추가할 수 있습니다(예: `myboot-7f889dd6d-n5z55`))


[.console-output]
[source,bash,subs="+quotes"]
----
Name:           myboot-7f889dd6d-n5z55
Namespace:      kubetut
Priority:       0
Node:           <none>
Labels:         app=myboot
                pod-template-hash=7f889dd6d
Annotations:    openshift.io/scc: restricted
Status:         Pending

Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type     Reason            Age        From               Message
  ----     ------            ----       ----               -------
  Warning  FailedScheduling  <unknown>  default-scheduler  #0/9 nodes are available: 9 node(s) had taints that the pod didn't tolerate.#
  Warning  FailedScheduling  <unknown>  default-scheduler  #0/9 nodes are available: 9 node(s) had taints that the pod didn't tolerate.#
----


클러스터의 노드 목록을 가져오겠습니다.

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc get nodes
----

[.console-output]
[source,bash]
----
NAME                                            STATUS   ROLES    AGE   VERSION
ip-10-0-136-107.eu-central-1.compute.internal   Ready    master   20h   v1.16.2
ip-10-0-140-186.eu-central-1.compute.internal   Ready    worker   20h   v1.16.2
ip-10-0-141-128.eu-central-1.compute.internal   Ready    worker   18h   v1.16.2
ip-10-0-146-109.eu-central-1.compute.internal   Ready    worker   18h   v1.16.2
ip-10-0-150-226.eu-central-1.compute.internal   Ready    worker   20h   v1.16.2
ip-10-0-155-122.eu-central-1.compute.internal   Ready    master   20h   v1.16.2
ip-10-0-162-206.eu-central-1.compute.internal   Ready    worker   20h   v1.16.2
ip-10-0-168-102.eu-central-1.compute.internal   Ready    master   20h   v1.16.2
ip-10-0-175-64.eu-central-1.compute.internal    Ready    worker   18h   v1.16.2
----

그리고 오염을 *제거*할 노드 하나를 선택합니다.

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl taint node ip-10-0-175-64.eu-central-1.compute.internal color:NoSchedule- 
----
IMPORTANT : nodename은 실제 조회된 nodenamewnd 하나를 입력합니다.

NOTE:  여기에 `-`를 추가하는 것은 문제의 오염을 제거한다는 의미입니다(`NoSchedule` 작업과 함께 `color`).


[.console-output]
[source,bash,subs="+attributes"]
----
node/{chosen-node}  untainted
----



이제 Terminal#2에서 새로 포함되지 않은 노드에 예약된 Pod가 표시됩니다.

[.console-output]
[source,bash,subs="+quotes"]
----
NAME                      READY   STATUS              AGE       NODE
myboot-7cbfbd9b89-hqx6h   0/1     #ContainerCreating#   20m   ip-10-0-175-64.eu-central-1.compute.internal
----


마지막으로 모든 노드의 taint 상태를 간단히 살펴보겠습니다.

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc describe nodes | egrep "Name:|Taints:"
----

[.console-output]
[source,bash]
----
Name:               ip-10-0-136-107.eu-central-1.compute.internal
Taints:             node-role.kubernetes.io/master:NoSchedule
Name:               ip-10-0-140-186.eu-central-1.compute.internal
Taints:             <none>
Name:               ip-10-0-141-128.eu-central-1.compute.internal
Taints:             color=blue:NoSchedule
Name:               ip-10-0-146-109.eu-central-1.compute.internal
Taints:             color=blue:NoSchedule
----


=== Restore Taint

노드(또는 이 경우 모든 노드)에 taint를 다시 추가합니다.


[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl taint nodes --all=true color=blue:NoSchedule --overwrite
----

[TIP]
====
모든 노드에 taint를 설정하는 것은 약간 엉성합니다.  원한다면 제거된 노드에만 taint를 설정하여 동일한 효과를 좀 더 우아하게 얻을 수 있습니다.  예를 들어,

----
kubectl taint node ip-10-0-140-186.eu-central-1.compute.internal color=blue:NoSchedule
----
====

taint가 변경되었음에도 불구하고 Pod가 계속 실행되고 있는지 살펴보세요. (이는 Pod 수명 주기에서 일회성 활동으로 예약하기 때문입니다.)


[.console-output]
[source,bash,subs="+macros,+attributes,+quotes"]
----
NAME                      READY   STATUS    AGE   NODE
myboot-7cbfbd9b89-bzhxw   1/1     #Running#   18m   ip-10-0-175-64.eu-central-1.compute.internal
----



=== Clean Up

myboot 배포를 취소하고 노드에 taint를 다시 추가합니다.

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl delete deployment 
----



== Tolerations

오염된 노드에 예약할 수 있도록 톨러레이션이 포함된 포드를 생성해 보겠습니다.

[source, yaml]
----
spec:
  tolerations:
  - key: "color"
    operator: "Equal"
    value: "blue"
    effect: "NoSchedule"
  containers:
  - name: myboot
    image: quay.io/rhdevelopers/myboot:v1
----




[.console-input]
[source,bash,subs="+macros,+attributes"]
----
cat <<EOF | oc create -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: myboot
  name: myboot
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myboot
  template:
    metadata:
      labels:
        app: myboot
    spec:
      tolerations:
      - key: "color"
        operator: "Equal"
        value: "blue"
        effect: "NoSchedule"
      containers:
      - name: myboot
        image: quay.io/rhdevelopers/myboot:v1
        ports:
          - containerPort: 8080
EOF
----


그런 다음 얼마 지나지 않아 감시 창에서 포드가 예약되고 실행 상태로 진행되는 것을 확인해야 합니다.



[.console-output]
[source,bash,subs="+quotes"]
----
NAME                      READY   STATUS    AGE     NODE
myboot-84b457458b-mbf9r   1/1     #Running#   3m18s   devnation-m02
----


이제 모든 노드에 오염이 포함되어 있더라도 color=blue 오염에 대한 허용 오차를 정의한 대로 Pod가 예약되고 실행됩니다.

=== Clean Up

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc delete deployment myboot
----



== `NoExecution` Taint

지금까지 'NoSchedule' 오염 효과를 살펴보았습니다. 이는 새로 생성된 포드가 우선적인 허용 범위를 갖지 않는 한 그곳에서 예약되지 않는다는 것을 의미합니다. 그러나 이미 실행 중이거나 예약된 포드가 있는 노드에 이 오염을 추가하면,  이 오염은 그들을 제거하지 못할 것입니다.


NoExecution 효과를 사용하여 이를 변경해 보겠습니다.  우선 이전 taint를 모두 제거해 보겠습니다.


[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl taint nodes --all=true color=blue:NoSchedule-
----



그런 다음 myboot의 다른 인스턴스를 배포합니다.(with no Tolerations):


[.console-input]
[source,bash,subs="+macros,+attributes"]
----
cat <<EOF | oc create -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: myboot
  name: myboot
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myboot
  template:
    metadata:
      labels:
        app: myboot
    spec:
      containers:
      - name: myboot
        image: quay.io/rhdevelopers/myboot:v1
        ports:
          - containerPort: 8080
EOF
----



Terminal#2에서 다음을 확인합니다.

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
watch oc get pod
----



[.console-output]
[source,bash]
----
NAME                      READY   STATUS    AGE   NODE
myboot-7cbfbd9b89-wpddg   1/1     Running   47s   devnation-m02
----


이제 포드가 실행 중인 노드를 찾아보겠습니다.

==Terminal#1에서 실행


[.console-input]
[source,bash,subs="+macros"]
----
NODE=$(kubectl get pod -o jsonpath='{.items[0].spec.nodeName}')
echo ${NODE}
----

NOTE:  `.items[0]`은 모든 포드를 요청하지만 목록에 요소가 하나만 포함된다는 것을 알고 있기 때문입니다.

[.console-output]
[source,bash]
----
"ip-10-0-146-109.eu-central-1.compute.internal"
----



==Terminal#1에서 수행

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl taint node ${NODE} color=blue:NoExecute
----


As soon as we do this, we should be able to watch this "rescheduling" occur in the {watch-terminal} watch

[tabs]
====
{watch-terminal}::
+
--

[.console-output]
[source,bash,subs="+quotes"]
----
NAME                      READY   STATUS              AGE   NODE
myboot-7cbfbd9b89-5t24z   0/1     #ContainerCreating#   16s   devnation
myboot-7cbfbd9b89-wpddg   1/1     #Terminating#         65m   devnation-m02
----

--
====

[NOTE]
====
If you have more nodes available then the Pod is terminated and deployed onto another node, if it is not the case, then the Pod will remain in `Pending` status.
====

=== Clean Up

[tabs]
====
Terminal 1::
+
--
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl delete -f apps/kubefiles/myboot-deployment.yml
----
--
====

And remove the NoExecute taint 

[tabs]
====
Terminal 1::
+
--
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl taint node pass:[${NODE}] color=blue:NoExecute-
----
--
====

== Affinity & Anti-Affinity

There is another way of changing where Pods are scheduled using Node/Pod Affinity and Anti-affinity.
You can create rules that not only ban where Pods can run but also to favor where they should be run.

In addition to creating affinities between Pods and Nodes, you can also create affinities between Pods.  You can decide that a group of Pods should be always be deployed together on the same node(s).
Reasons such as significant network communication between Pods and you want to avoid external network calls or perhaps shared storage devices.

=== Node Affinity

:quick-open-file: myboot-node-affinity.yml

Let's deploy a new pod with a node affinity.  Take a look at `{quick-open-file}` (relevant section shown below)

include::partial$tip_vscode_quick_open.adoc[]

[source, yaml]
.{quick-open-file}
----
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution: #<.>
        nodeSelectorTerms:
        - matchExpressions:
          - key: color
            operator: In
            values:
            - blue #<.>
      containers:
      - name: myboot
        image: quay.io/rhdevelopers/myboot:v1
----
<.> This key highlights that what's follows must be used during scheduling but not a factor once a pod is executing
<.> The `matchExpressions` is saying this pod has affinity for any node with a `color` in the value set `blue`

Now let's deploy this

[tabs]
====
Terminal 1::
+
--
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl apply -f apps/kubefiles/myboot-node-affinity.yml
----
--
====

And we'll see in our watch window the pod in a pending state

[tabs]
====
{watch-terminal}::
+
--
[.console-output]
[source,bash,subs="+macros,+attributes,+quotes"]
----
NAME                      READY   STATUS    AGE   NODE
myboot-546d4d9b45-7vgfc   0/1     #Pending#   6s    <none>
----
--
====

Let's create a *label* on a node matching the affinity expression:

[tabs]
====
Terminal 1 - Minikube::
+
--
include::partial$affinity_label.adoc[tags=**;!*;minikube]
--
Terminal 1 - OpenShift::
+
--
include::partial$affinity_label.adoc[tags=**;!*;openshift]
--
====

And then in the watch window the output should change to:

[tabs]
====
{watch-terminal}::
+
--
[.console-output]
[source,bash,subs="+macros,+attributes,+quotes"]
----
NAME                      READY   STATUS              AGE   NODE
myboot-546d4d9b45-7vgfc   0/1     #ContainerCreating#   15m   devnation-m02
----
--
====


Let's delete the label from the node that the pod is running on

[tabs]
====
Terminal 1::
+
--
First find the node the pod is running on

include::partial$find_node_for_pod.adoc[]

and then remove the color label from it

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl label nodes pass:[${NODE}] color-
----
--
====

And notice the that watch output is *unchanged* and if running, the pod will continue to run

[tabs]
====
{watch-terminal}::
+
--
[.console-output]
[source,bash]
----
NAME                      READY   STATUS    AGE   NODE
myboot-546d4d9b45-7vgfc   1/1     Running   22m   devnation-m02
----
--
====

Since we used the `requiredDuringSchedulingIgnoredDuringExecution` in the deployment spec for our pod, we got our affinity to work like taints (in the previous section) worked, namely, that the rule is set during the scheduling phase but ignore after that (i.e. once executing).  Therefore the Pod is not removed in our case.

This is an example of a _hard_ rule:

.Hard Rule
****
If the Kubernetes scheduler does not find any node with the required label then the Pod reminds in _Pending_ state.
****

There is also a way to create a _soft_ rule:

.Soft Rule
****
The Kubernetes scheduler attempts to match the rules but if it can.  However, if it can't then the Pod is scheduled to any node.  
****

Consider the example below:

[.console-output]
[source,yaml,subs="+macros,+attributes,+quotes"]
----
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution: #<.>
      - weight: 1
        preference:
          matchExpressions:
          - key: color
            operator: In
            values:
            - blue
----
<.> You can see the use of the word _preferred_ vs _required_.

==== Clean Up

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl delete -f apps/kubefiles/myboot-node-affinity.yml
----

=== Pod Affinity/Anti-Affinity

:quick-open-file: myboot-pod-affinity.yml

Let's deploy a new pod with a Pod Affinity.  See this relevant part of `{quick-open-file}`.  

include::partial$tip_vscode_quick_open.adoc[]

[source, yaml]
.{quick-open-file}
----
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - topologyKey: kubernetes.io/hostname # <1>
        labelSelector: 
          matchExpressions:
          - key: app
            operator: In
            values:
            - myboot # <2>
  containers:
----
<1> The node label key. If two nodes are labeled with this key and have identical values, the scheduler treats both nodes as being in the same topology. In this case, `hostname` is a label that is different for each node.
<2> The affinity is with Pods labeled with `app=myboot`.

[tabs]
====
Terminal 1::
+
--
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl apply -f apps/kubefiles/myboot-pod-affinity.yml
----

[.console-output]
[source,bash]
----
NAME                      READY  STATUS   AGE    NODE
myboot2-7c5f46cbc9-hwm2v  0/1    Pending  5h38m  <none>
----
--
====

The `myboot2` Pod is pending as couldn't find any Pod matching the affinity rule.

To address this, let's deploy a  `myboot` application labeled with `app=myboot`.

[tabs]
====
Terminal 1::
+
--
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl apply -f apps/kubefiles/myboot-deployment.yml
----
--
====

And we'll see that both start up, and run on _the same node_

[tabs]
====
{watch-terminal}::
+
--
[.console-output]
[source,bash,subs="+quotes"]
----
NAME                      READY  STATUS             AGE    NODE
myboot-7cbfbd9b89-267k6   0/1    ContainerCreating  5s     #devnation-m02#
myboot2-7c5f46cbc9-hwm2v  0/1    ContainerCreating  5h45m  #devnation-m02#
----
--
====

[TIP]
====
What you've just seen is a _hard_ rule, you can use a "soft" rules as well in Pod Affinity.

[.console-output]
[source, yaml, subs="+quotes"]
----
spec:
  affinity:
    podAntiAffinity:
      #preferredDuringSchedulingIgnoredDuringExecution:#
      - weight: 1
        podAffinityTerm:
          topologyKey: kubernetes.io/hostname 
          labelSelector:
            matchExpressions:  
            - key: app
              operator: In
              values:
              - myboot   
----
====

*Anti-affinity* is used to insure that two Pods do NOT run together on the same node.

:quick-open-file: myboot-pod-antiaffinity.yaml

Let's add another pod.  Open `{quick-open-file}` and focus on the following part

include::partial$tip_vscode_quick_open.adoc[]

[.console-output]
[source, yaml]
.{quick-open-file}
----
spec:
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - topologyKey: kubernetes.io/hostname
        labelSelector: 
          matchExpressions:
          - key: app
            operator: In
            values:
            - myboot
----

This basically says that this pod should not be scheduled on any individual node (`topologyKey: kubernetes.io/hostname`) that has a pod with the `app=myboot` label.

Deploy a myboot3 with the above anti-affinity rule

[tabs]
====
Terminal 1::
+
--
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl apply -f apps/kubefiles/myboot-pod-antiaffinity.yaml
----
--
====

And then notice what happens in the watch window

[tabs]
====
{watch-terminal}::
+
--
[.console-output]
[source,bash,subs="+macros,+attributes,+quotes"]
----
NAME                      READY  STATUS             AGE    NODE
myboot-7cbfbd9b89-267k6   1/1    Running            10m    devnation-m02
myboot2-7c5f46cbc9-hwm2v  1/1    Running            5h56m  devnation-m02
myboot3-6f95c866f6-7kvdw  0/1    ContainerCreating  6s     #devnation# 
----
--
====

As you can see from the highlight, the `myboot3` Pod is deployed in a different node than the `myboot` Pod

==== Clean Up

[tabs]
====
Terminal 1::
+
--
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl delete -f apps/kubefiles/myboot-pod-affinity.yml
kubectl delete -f apps/kubefiles/myboot-pod-antiaffinity.yaml
kubectl delete -f apps/kubefiles/myboot-deployment.yml
----
--
====

