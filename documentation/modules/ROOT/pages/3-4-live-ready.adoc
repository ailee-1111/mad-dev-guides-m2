
= Kubernetes를 통한 애플리케이션 고가용성

== 고가용성 애플리케이션 배포 개념
고가용성 (HA)은 애플리케이션을 더욱 강력하게 만들고 런타임 오류를 방지하는 것을 목표로 합니다. HA 기술을 구현하면 사용자가 애플리케이션을 완전히 사용할 수 없게 될 가능성이 줄어듭니다.

일반적으로 HA는 다음과 같은 상황에서 애플리케이션을 오류로부터 보호할 수 있습니다.

* 애플리케이션 버그 형태의 그 자체에서

* 네트워킹 문제와 같은 환경에서

* 클러스터 리소스를 소모하는 다른 애플리케이션에서

또한 HA 사례는 메모리 누수가 있는 애플리케이션과 같은 애플리케이션으로부터 클러스터를 보호할 수 있습니다.

== 신뢰성 있는 애플리케이션 작성
기본적으로 클러스터 수준 HA 툴링은 최악의 시나리오를 완화합니다. HA는 애플리케이션 수준 문제를 해결하는 용도로는 사용할 수 없지만 개발자의 완화 기능을 강화합니다. 애플리케이션 보안은 안정성을 위해 필요하지만 별개의 문제입니다.

Kubernetes가 장애 시나리오를 가장 잘 처리할 수 있도록 애플리케이션이 클러스터와 함께 작동해야 합니다. Kubernetes는 애플리케이션에서 다음 동작을 예상합니다.

* 재시작 허용

* 시작, 준비 상태, 활성 프로브 등의 상태 프로브에 응답

* 여러 개의 동시 인스턴스 지원

* 잘 정의되고 올바르게 작동하는 리소스 사용

* 제한된 권한으로 작동

클러스터는 앞의 동작이 없는 애플리케이션을 실행할 수 있지만 이러한 동작이 있는 애플리케이션은 Kubernetes가 제공하는 안정성 및 HA 기능을 더 잘 사용합니다.

대부분의 HTTP 기반 애플리케이션은 애플리케이션 상태를 확인하는 엔드포인트를 제공합니다. 클러스터는 이 엔드포인트를 관찰하고 애플리케이션의 잠재적 문제를 완화하도록 구성할 수 있습니다.

애플리케이션은 이러한 엔드포인트를 제공해야 합니다. 개발자는 애플리케이션에서 상태를 확인하는 방법을 결정해야 합니다.

예를 들어 애플리케이션이 데이터베이스 연결에 종속된 경우 데이터베이스에 연결할 수 있는 경우에만 애플리케이션이 정상 상태로 응답할 수 있습니다. 그러나 데이터베이스 연결을 설정하는 모든 애플리케이션에 이러한 확인이 필요한 것은 아닙니다. 이 결정은 개발자의 재량에 따릅니다.

== Kubernetes 애플리케이션 안정성
애플리케이션 포드가 충돌하면 요청에 응답할 수 없습니다. 구성에 따라 클러스터는 포드를 자동으로 다시 시작할 수 있습니다. 포드가 충돌하지 않고 애플리케이션이 실패하면 포드는 요청을 수신하지 않습니다. 그러나 클러스터는 적절한 상태 프로브를 통해서만 이 작업을 수행할 수 있습니다.

Kubernetes는 다음 HA 기술을 사용하여 애플리케이션 안정성을 개선합니다.

* 포드 다시 시작: 포드에서 재시작 정책을 구성하면 클러스터가 오작동하는 애플리케이션 인스턴스를 다시 시작합니다.

* 프로빙: 클러스터는 상태 프로브를 사용하여 애플리케이션이 요청에 응답할 수 없는 시기를 파악하고 문제를 완화하기 위해 자동으로 조치를 취할 수 있습니다.

* 수평적 확장: 애플리케이션 로드가 변경되면 클러스터는 로드에 맞게 복제본 수를 확장할 수 있습니다.



= 애플리케이션 상태 프로브

== Kubernetes 프로브
상태 프로브는 강력한 클러스터를 유지 관리하는 데 있어 중요한 부분입니다. 프로브 를 사용하면 클러스터에서 응답을 반복적으로 프로빙하여 애플리케이션의 상태를 확인할 수 있습니다.

상태 프로브 집합은 다음 작업을 수행하는 클러스터의 기능에 영향을 줍니다.

* 실패한 포드를 자동으로 다시 시작하여 충돌 완화

* 정상 포드에만 요청을 전송하여 페일오버 및 부하 분산

* 포드가 실패하는지 여부와 시기를 확인하여 모니터링

* 새 복제본이 요청을 수신할 준비가 된 시점을 확인하여 확장

== 프로브 엔드포인트 작성
애플리케이션 개발자는 애플리케이션 개발 중에 Health Probe 엔드포인트를 코딩해야 합니다. 이러한 엔드포인트는 애플리케이션의 상태를 결정합니다. 예를 들어 데이터 기반 애플리케이션은 데이터베이스에 연결할 수 있는 경우에만 성공적인 Health Probe를 보고할 수 있습니다.

클러스터에서 자주 호출하므로 Health Probe 엔드포인트를 신속하게 수행해야 합니다. 엔드포인트는 복잡한 데이터베이스 쿼리 또는 많은 네트워크 호출을 수행하면 안 됩니다.

== 프로브 유형
Kubernetes는 시작, 준비, 활성 상태의 프로브 유형을 제공합니다. 애플리케이션에 따라 이러한 유형 중 하나 이상을 구성할 수 있습니다.

== 준비 프로브
준비 프로브 는 애플리케이션이 요청을 처리할 준비가 되었는지 여부를 확인합니다. 준비 프로브가 실패하면 Kubernetes는 서비스 리소스에서 포드의 IP 주소를 제거하여 클라이언트 트래픽이 애플리케이션에 도달하지 못하게 합니다.

준비 프로브는 애플리케이션에 영향을 줄 수 있는 일시적인 문제를 감지하는 데 도움이 됩니다. 예를 들어 애플리케이션은 초기 네트워크 연결을 설정하고, 캐시에 파일을 로드하거나, 완료하는 데 시간이 걸리는 초기 작업을 수행해야 하기 때문에 시작할 때 일시적으로 사용하지 못할 수 있습니다. 애플리케이션은 때때로 클라이언트에서 일시적으로 사용할 수 없는 긴 배치 작업을 실행해야 할 수 있습니다.

Kubernetes는 애플리케이션이 실패한 후에도 프로브를 계속 실행합니다. 프로브가 다시 성공하면 Kubernetes가 포드의 IP 주소를 서비스 리소스에 다시 추가하고 요청이 포드로 다시 전송됩니다.

이러한 경우 준비 프로브는 일시적인 문제를 해결하고 애플리케이션 가용성을 개선합니다.

== 활성 프로브
준비 프로브와 마찬가지로 활성 프로브 는 애플리케이션 수명 중에 호출됩니다. 활성 프로브는 애플리케이션 컨테이너 상태가 정상인지 확인합니다. 애플리케이션이 활성 프로브에 어느 횟수만큼 실패하면 클러스터는 재시작 정책에 따라 포드를 다시 시작합니다.

시작 프로브와 달리 활성 프로브는 애플리케이션의 초기 시작 프로세스 후에 호출됩니다. 일반적으로 이 완화 방법을 사용하려면 포드를 다시 시작하거나 다시 생성해야 합니다.

== 시작 프로브
시작 프로브 는 애플리케이션 시작이 완료되는 시점을 확인합니다. 활성 프로브와 달리 시작 프로브는 프로브가 성공한 후에 호출되지 않습니다. 구성 가능한 제한 시간 후에 시작 프로브가 성공하지 못하면 restartPolicy 값에 따라 포드가 다시 시작됩니다.

시작 시간이 긴 애플리케이션에 시작 프로브를 추가하는 것이 좋습니다. 시작 프로브를 사용하면 활성 프로브를 짧게 유지하고 응답할 수 있습니다.

== 테스트 유형
프로브를 정의할 때 수행할 테스트 유형 중 하나를 지정해야 합니다.

* *HTTP GET*
프로브가 실행될 때마다 클러스터는 지정된 HTTP 엔드포인트에 요청을 보냅니다. 요청이 200 및 399 사이의 HTTP 응답 코드로 응답하면 테스트가 성공한 것으로 간주됩니다. 다른 응답으로 인해 테스트가 실패합니다.

* *컨테이너 명령*
프로브가 실행될 때마다 클러스터는 컨테이너에서 지정된 명령을 실행합니다. 명령이 0 상태 코드와 함께 종료되면 테스트가 성공한 것입니다. 다른 상태 코드로 인해 테스트가 실패합니다.

* *TCP 소켓*
프로브가 실행될 때마다 클러스터는 컨테이너에 대한 소켓을 열려고 시도합니다. 연결이 설정된 경우에만 테스트가 성공합니다.

* *타이밍 및 임계값*
모든 유형의 프로브에는 타이밍 변수가 포함됩니다. period seconds 변수는 프로브가 실행되는 빈도를 정의합니다. failure threshold 는 프로브 자체가 실패하기 전에 필요한 실패한 시도 횟수를 정의합니다.

예를 들어 실패 임계값이 3 이고 period seconds가 5 인 프로브는 전체 프로브가 실패하기 전에 최대 세 번 실패할 수 있습니다. 이 프로브 구성을 사용하면 문제가 완화되기 전에 10초 동안 문제가 존재할 수 있습니다. 그러나 프로브를 너무 자주 실행하면 리소스가 낭비될 수 있습니다. 



= Liveness & Readiness

Make sure you are in the correct namespace:

:section-k8s: liveready
:set-namespace: myspace

include::partial$set-context.adoc[]

다른 것이 배포되지 않았는지 확인하세요.

[#no-resources-live-ready]
[.console-input]
[source, bash]
----
kubectl get all
----

[.console-output]
[source.bash]
----
No resources found in myspace namespace.
----

:quick-open-file: myboot-deployment-live-ready.yml

Now we're going to deploy our application with a Liveness and Readiness probe set.  Take a look at `{quick-open-file}`

include::partial$tip_vscode_quick_open.adoc[]

[.console-output]
[source,yaml]
.{quick-open-file}
----
include::example$myboot-deployment-live-ready.yml[]
----

Now apply this deployment with the following command

[#create-app-live-ready]
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl apply -f apps/kubefiles/myboot-deployment-live-ready.yml
----

Describe the deployment:

:describe-deployment-name: myboot
:section-k8s: live-ready

include::partial$describe-deployment.adoc[]

[.console-output]
[source.bash]
----
...
    Image:      quay.io/rhdevelopers/myboot:v1
    Port:       8080/TCP
    Host Port:  0/TCP
    Limits:
      cpu:     1
      memory:  400Mi
    Requests:
      cpu:        250m
      memory:     300Mi
    Liveness:     http-get http://:8080/ delay=10s timeout=2s period=5s #success=1 #failure=3
    Readiness:    http-get http://:8080/health delay=10s timeout=1s period=3s #success=1 #failure=3
...    
----

Deploy a Service:

[#deploy-service-live-ready]
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl apply -f apps/kubefiles/myboot-service.yml
----

:section-k8s: liveready
:service-exposed: myboot
include::partial$env-curl.adoc[]

And run loop script:

include::partial$loop.adoc[]

Change the image:

[#change-deployment-v2-live-ready]
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl set image deployment/myboot myboot=quay.io/rhdevelopers/myboot:v2
----

And notice the error free rolling update:

[.console-output]
[source.bash]
----
Aloha from Spring Boot! 131 on myboot-845968c6ff-k4rvb
Aloha from Spring Boot! 134 on myboot-845968c6ff-9wvt9
Aloha from Spring Boot! 122 on myboot-845968c6ff-9824z
Bonjour from Spring Boot! 0 on myboot-8449d5468d-m88z4
Bonjour from Spring Boot! 1 on myboot-8449d5468d-m88z4
Aloha from Spring Boot! 135 on myboot-845968c6ff-9wvt9
Aloha from Spring Boot! 133 on myboot-845968c6ff-k4rvb
Aloha from Spring Boot! 137 on myboot-845968c6ff-9wvt9
Bonjour from Spring Boot! 3 on myboot-8449d5468d-m88z4
----

Look at the Endpoints to see which pods are part of the Service:

[#get-endpoints-before]
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl get endpoints myboot -o json | jq '.subsets[].addresses[].ip'
----

These are the Pod IPs that have passed their readiness probes:

[.console-output]
[source.bash]
----
"10.129.2.40"
"10.130.2.37"
"10.130.2.38"
----

== Readiness Probe

Exec into a single Pod and change its readiness flag:

[#misbehave-app-live-ready]
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl exec -it myboot-845968c6ff-k5lcb /bin/bash
----

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
curl localhost:8080/misbehave
exit
----

See that the pod is no longer Ready:

[.console-output]
[source.bash]
----
NAME                      READY   STATUS    RESTARTS   AGE
myboot-845968c6ff-9wshg   1/1     Running   0          11m
myboot-845968c6ff-k5lcb   0/1     Running   0          12m
myboot-845968c6ff-zsgx2   1/1     Running   0          11m
----

Now check the Endpoints:

[#get-endpoints-after]
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl get endpoints myboot -o json | jq '.subsets[].addresses[].ip'
----

And that pod is now missing from the Service's loadbalancer:

[.console-output]
[source.bash]
----
"10.130.2.37"
"10.130.2.38"
----

Which is also self-evident in the curl loop:

[.console-output]
[source.bash]
----
Aloha from Spring Boot! 845 on myboot-845968c6ff-9wshg
Aloha from Spring Boot! 604 on myboot-845968c6ff-zsgx2
Aloha from Spring Boot! 846 on myboot-845968c6ff-9wshg
----

== Liveness Probe

[#change-deployment-v3-live-ready]
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl set image deployment/myboot myboot=quay.io/rhdevelopers/myboot:v3
----

Let the rollout finish to completion across all 3 replicas:

[.console-output]
[source.bash]
----
watch kubectl get pods
NAME                      READY   STATUS    RESTARTS   AGE
myboot-56659c9d69-6sglj   1/1     Running   0          2m2s
myboot-56659c9d69-mdllq   1/1     Running   0          97s
myboot-56659c9d69-zjt6q   1/1     Running   0          72s
----

And as seen in the curl loop/poller:

[.console-output]
[source.bash]
----
Jambo from Spring Boot! 40 on myboot-56659c9d69-mdllq
Jambo from Spring Boot! 26 on myboot-56659c9d69-zjt6q
Jambo from Spring Boot! 71 on myboot-56659c9d69-6sglj
----

Edit the Deployment to point to the /alive URL:

include::partial$tip_vscode_kube_editor.adoc[]

[#change-liveness-v3-live-ready]
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl edit deployment myboot
----

And change the liveness probe:

[.console-output]
[source.bash]
----
...
    spec:
      containers:
      - image: quay.io/rhdevelopers/myboot:v3
        imagePullPolicy: Always
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /alive
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 5
          successThreshold: 1
          timeoutSeconds: 2
        name: myboot
...
----

Save and close the editor, allowing that change to rollout:

[.console-input]
[source,bash]
----
watch kubectl get pods
----

[.console-output]
[source,bash]
----
NAME                      READY   STATUS        RESTARTS   AGE
myboot-558b4f8678-nw762   1/1     Running       0          59s
myboot-558b4f8678-qbrgc   1/1     Running       0          81s
myboot-558b4f8678-z7f9n   1/1     Running       0          36s
----

Now pick one of the pods, `exec` into it and shoot it:

[#shot-v3-live-ready]
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl exec -it myboot-558b4f8678-qbrgc /bin/bash
----

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
curl localhost:8080/shot
----

And you will see it get restarted:

[.console-output]
[source.bash]
----
NAME                      READY   STATUS    RESTARTS   AGE
myboot-558b4f8678-nw762   1/1     Running   0          4m7s
myboot-558b4f8678-qbrgc   1/1     Running   1          4m29s
myboot-558b4f8678-z7f9n   1/1     Running   0          3m44s
----

Plus, your exec will be terminated:

[.console-input]
[source,bash]
----
kubectl exec -it myboot-558b4f8678-qbrgc /bin/bash
----

[.console-output]
[source.bash]
----
curl localhost:8080/shot
----

[.console-output]
[source.bash]
----
I have been shot in the head1000610000@myboot-558b4f8678-qbrgc:/app$ command terminated with exit code 137
----

And your end-users will not see any errors:

[.console-output]
[source.bash]
----
Jambo from Spring Boot! 174 on myboot-558b4f8678-z7f9n
Jambo from Spring Boot! 11 on myboot-558b4f8678-qbrgc
Jambo from Spring Boot! 12 on myboot-558b4f8678-qbrgc
Jambo from Spring Boot! 206 on myboot-558b4f8678-nw762
Jambo from Spring Boot! 207 on myboot-558b4f8678-nw762
Jambo from Spring Boot! 175 on myboot-558b4f8678-z7f9n
Jambo from Spring Boot! 176 on myboot-558b4f8678-z7f9n
----

== Clean up

[#cleanup-live-ready]
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl delete deployment myboot
----

== Startup Probe

Some applications require an additional startup time on their first initialization.

It might be tricky to fit this scenario into the liveness/readiness probes as you need to configure them for their normal behaviour to detect abnormalities during the running time and moreover covering the long start up time.

:quick-open-file: myboot-deployment-live-ready-aggressive.yml

For instance, what if we had an application that might deadlock and we want to catch such issues immediately, we might have liveness and readiness probes that look like in `apps/kubefiles/{quick-open-file}`

include::partial$tip_vscode_quick_open.adoc[]

[.console-output]
[source,yaml]
.{quick-open-file}
----
include::example$myboot-deployment-live-ready-aggressive.yml[]
----

Then apply that deployment

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl apply -f apps/kubefiles/myboot-deployment-live-ready-aggressive.yml
----

As we'll see from the pod watch, the pods are continually getting restarted, sometimes after it successfully boots up (because kubelet schedules for restart) and this is due to the startup time of SpringBoot.


[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl describe pods
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
Events:
  Type     Reason     Age                 From               Message
  ----     ------     ----                ----               -------
  Normal   Scheduled  96s                 default-scheduler  Successfully assigned myspace/myboot-849ccd6948-8vrfq to devnation
  Normal   Pulled     92s                 kubelet            Successfully pulled image "quay.io/rhdevelopers/myboot:v1" in 3.295180194s
  Normal   Created    55s (x2 over 92s)   kubelet            Created container myboot
  Normal   Started    55s (x2 over 92s)   kubelet            Started container myboot
  Normal   Pulled     55s                 kubelet            Successfully pulled image "quay.io/rhdevelopers/myboot:v1" in 3.289395484s
  Warning  Unhealthy  52s (x4 over 90s)   kubelet            Liveness probe failed: Get "http://172.17.0.4:8080/alive": dial tcp 172.17.0.4:8080: connect: connection refused
  Normal   Killing    52s (x2 over 88s)   kubelet            Container myboot failed liveness probe, will be restarted
  Normal   Pulling    22s (x3 over 95s)   kubelet            Pulling image "quay.io/rhdevelopers/myboot:v1"
  Warning  Unhealthy  19s (x10 over 88s)  kubelet            Readiness probe failed: Get "http://172.17.0.4:8080/health": dial tcp 172.17.0.4:8080: connect: connection refused
----

*Startup probes* fix this problem, as once the startup probe has succeeded, the rest of the probes take over, but until the startup probe passes, neither the liveness nor the readiness probes can run. 

:quick-open-file: myboot-deployment-startup-live-ready.yml

`{quick-open-file}` is an example of a deployment with just such a probe

include::partial$tip_vscode_quick_open.adoc[]

[.console-output]
[source,yaml]
.{quick-open-file}
----
include::example$myboot-deployment-startup-live-ready.yml[]
----

You'll see the difference is this section

[.console-output]
[source,yaml]
----
        startupProbe:
          httpGet:
            path: /alive
            port: 8080
          failureThreshold: 6
          periodSeconds: 5
          timeoutSeconds: 1
----

Then apply that deployment

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl apply -f apps/kubefiles/myboot-deployment-startup-live-ready.yml
----

The startup probe waits for 30 seconds (`5 * 6`) to startup the application.  Notice, too, that the delay on the liveness and readiness checks has gone down to 0.

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
watch kubectl get pods
----

[.console-output]
[source.bash]
----
NAME                      READY   STATUS    RESTARTS   AGE
myboot-579cc5cc47-2bk5p   0/1     Running   0          67s
----

Eventually your curl loop should show the pod running

----
Aloha from Spring Boot! 18 on myboot-849ccd6948-8vrfq
Aloha from Spring Boot! 19 on myboot-849ccd6948-8vrfq
Aloha from Spring Boot! 20 on myboot-849ccd6948-8vrfq
Aloha from Spring Boot! 21 on myboot-849ccd6948-8vrfq
----

Let's show that the liveness probe has taken over.
Now pick one of the pods, `exec` into it and shoot it:

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl exec -it myboot-558b4f8678-qbrgc /bin/bash
----

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
curl localhost:8080/shot
----

And you will see it get restarted.


Describe the pod to get the statistics of probes:

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl describe pod myboot-579cc5cc47-2bk5p
----

[.console-output]
[source.yaml]
----
Limits:
  cpu:     1
  memory:  400Mi
Requests:
  cpu:        250m
  memory:     300Mi
Liveness:     http-get http://:8080/ delay=10s timeout=2s period=5s #success=1 #failure=3
Readiness:    http-get http://:8080/health delay=10s timeout=1s period=3s #success=1 #failure=3
Startup:      http-get http://:8080/alive delay=0s timeout=1s period=5s #success=1 #failure=12
Environment:  <none>
Mounts:
----

== Clean Up

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl delete deployment myboot
kubectl delete svc myboot
----

